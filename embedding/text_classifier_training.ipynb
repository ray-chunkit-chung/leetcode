{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Prepare data\n",
    "\n",
    "- NHK program list text and genres (too few)\n",
    "- MARC-ja (not available)\n",
    "- JCoLA https://github.com/osekilab/JCoLA <-- we are using this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_nhk_programs(url: str) -> dict:\n",
    "#     \"\"\"\n",
    "#     NHK番組表を取得する\n",
    "#     \"\"\"\n",
    "\n",
    "#     try:\n",
    "#         response = requests.get(url)\n",
    "#         response.raise_for_status()\n",
    "#         data = response.json()\n",
    "#     except:\n",
    "#         data = None\n",
    "#     return data\n",
    "\n",
    "\n",
    "# def cast_json_to_pandas(json_data):\n",
    "#     \"\"\"\n",
    "#     JSONデータをPandasのDataFrameに変換する\n",
    "#     \"\"\"\n",
    "#     if json_data is None:\n",
    "#         return None\n",
    "#     return pd.json_normalize(json_data[\"list\"][\"g1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://api.nhk.or.jp/v2/pg/genre/{area}/{service}/{genre}/{date}.json?key={apikey}\n",
    "\n",
    "# apikey = os.environ.get(\"NHK_API_KEY\", None)\n",
    "# area = \"130\"  # 東京\n",
    "# service = \"g1\"  # NHK総合\n",
    "# start_date = \"2024-04-24\"  # 開始日\n",
    "# end_date = \"2024-11-24\"  # 終了日\n",
    "\n",
    "# get_list_of_dates = (\n",
    "#     lambda start_date, end_date: pd.date_range(start_date, end_date)\n",
    "#     .strftime(\"%Y-%m-%d\")\n",
    "#     .tolist()\n",
    "# )\n",
    "\n",
    "# dates = get_list_of_dates(start_date, end_date)\n",
    "\n",
    "# nhk_programs_json = map(\n",
    "#     lambda date: get_nhk_programs(\n",
    "#         f\"https://api.nhk.or.jp/v2/pg/list/{area}/{service}/{date}.json?key={apikey}\"\n",
    "#     ),\n",
    "#     dates,\n",
    "# )\n",
    "# nhk_programs_json = filter(lambda x: x is not None, nhk_programs_json)\n",
    "# df = pd.concat(map(cast_json_to_pandas, nhk_programs_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = \"../local/data/JCoLA-main/data/jcola-v1.0/in_domain_train-v1.0.tsv\"\n",
    "# df = pd.read_csv(data_path, sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Featrue engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(\"../local/data\", exist_ok=True)\n",
    "# df.to_csv(\"../local/data/nhk_programs.csv\", index=False)\n",
    "# df = pd.read_csv(\"../local/data/nhk_programs.csv\", dtype=str).dropna()\n",
    "\n",
    "data_path = \"../local/data/JCoLA-main/data/jcola-v1.0/in_domain_train-v1.0.tsv\"\n",
    "df = pd.read_csv(data_path, sep=\"\\t\")\n",
    "\n",
    "# Text feature extraction\n",
    "model_name = \"oshizo/sbert-jsnli-luke-japanese-base-lite\"\n",
    "# model_name = \"studio-ousia/luke-japanese-base-lite\"\n",
    "encoder = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "# text_features = [\"title\", \"subtitle\", \"content\", \"act\"]\n",
    "text_features = [\"sentence\"]\n",
    "\n",
    "for i in text_features:\n",
    "    embeddings = pd.DataFrame(\n",
    "        encoder.embed_documents(df[i]), index=df.index\n",
    "    ).add_prefix(f\"embed_{i}_\")\n",
    "    df = df.join(embeddings, how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embed_sentence_0</th>\n",
       "      <th>embed_sentence_1</th>\n",
       "      <th>embed_sentence_2</th>\n",
       "      <th>embed_sentence_3</th>\n",
       "      <th>embed_sentence_4</th>\n",
       "      <th>embed_sentence_5</th>\n",
       "      <th>embed_sentence_6</th>\n",
       "      <th>embed_sentence_7</th>\n",
       "      <th>embed_sentence_8</th>\n",
       "      <th>embed_sentence_9</th>\n",
       "      <th>...</th>\n",
       "      <th>embed_sentence_759</th>\n",
       "      <th>embed_sentence_760</th>\n",
       "      <th>embed_sentence_761</th>\n",
       "      <th>embed_sentence_762</th>\n",
       "      <th>embed_sentence_763</th>\n",
       "      <th>embed_sentence_764</th>\n",
       "      <th>embed_sentence_765</th>\n",
       "      <th>embed_sentence_766</th>\n",
       "      <th>embed_sentence_767</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.779722</td>\n",
       "      <td>-0.159863</td>\n",
       "      <td>0.390812</td>\n",
       "      <td>0.163809</td>\n",
       "      <td>0.441645</td>\n",
       "      <td>-0.422382</td>\n",
       "      <td>0.294479</td>\n",
       "      <td>0.004554</td>\n",
       "      <td>-0.151662</td>\n",
       "      <td>0.318913</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.282139</td>\n",
       "      <td>-0.167588</td>\n",
       "      <td>0.326852</td>\n",
       "      <td>-0.147425</td>\n",
       "      <td>0.16059</td>\n",
       "      <td>0.037417</td>\n",
       "      <td>-0.125623</td>\n",
       "      <td>0.472888</td>\n",
       "      <td>-0.110388</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   embed_sentence_0  embed_sentence_1  embed_sentence_2  embed_sentence_3  \\\n",
       "0          0.779722         -0.159863          0.390812          0.163809   \n",
       "\n",
       "   embed_sentence_4  embed_sentence_5  embed_sentence_6  embed_sentence_7  \\\n",
       "0          0.441645         -0.422382          0.294479          0.004554   \n",
       "\n",
       "   embed_sentence_8  embed_sentence_9  ...  embed_sentence_759  \\\n",
       "0         -0.151662          0.318913  ...           -0.282139   \n",
       "\n",
       "   embed_sentence_760  embed_sentence_761  embed_sentence_762  \\\n",
       "0           -0.167588            0.326852           -0.147425   \n",
       "\n",
       "   embed_sentence_763  embed_sentence_764  embed_sentence_765  \\\n",
       "0             0.16059            0.037417           -0.125623   \n",
       "\n",
       "   embed_sentence_766  embed_sentence_767  label  \n",
       "0            0.472888           -0.110388      1  \n",
       "\n",
       "[1 rows x 769 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose training features\n",
    "# training_features = [i for i in df.columns if \"embed_\" in i]\n",
    "# df = df[training_features + [\"label\"]]\n",
    "\n",
    "# Target label data clearning\n",
    "# df[\"genres\"] = df[\"genres\"].map(eval)\n",
    "# df = df.explode(\"genres\").reset_index(drop=True)\n",
    "\n",
    "# Choose training features\n",
    "training_features = [i for i in df.columns if \"embed_\" in i]\n",
    "target_label = \"label\"\n",
    "df = df[training_features + [target_label]]\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score  0.8643179765130985\n",
      "test score  0.8229768786127167\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.18      0.26       236\n",
      "           1       0.85      0.96      0.90      1148\n",
      "\n",
      "    accuracy                           0.82      1384\n",
      "   macro avg       0.65      0.57      0.58      1384\n",
      "weighted avg       0.78      0.82      0.79      1384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# train classifier by logitic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X = df[training_features]\n",
    "y = df[target_label]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"train score \", clf.score(X_train, y_train))\n",
    "print(\"test score \", clf.score(X_test, y_test))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4621, number of negative: 914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056797 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 5535, number of used features: 768\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.834869 -> initscore=1.620536\n",
      "[LightGBM] [Info] Start training from score 1.620536\n",
      "train score  0.9998193315266486\n",
      "test score  0.8236994219653179\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.05      0.08       236\n",
      "           1       0.83      0.98      0.90      1148\n",
      "\n",
      "    accuracy                           0.82      1384\n",
      "   macro avg       0.60      0.52      0.49      1384\n",
      "weighted avg       0.75      0.82      0.76      1384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train text classifier with light boost\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "clf = LGBMClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "print(\"train score \", clf.score(X_train, y_train))\n",
    "print(\"test score \", clf.score(X_test, y_test))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using HuggingFace api for feature extraction\n",
    "\n",
    "# model_id = \"oshizo/sbert-jsnli-luke-japanese-base-lite\"\n",
    "# hf_token = os.environ.get(\"HUGGINGFACE_API_KEY_READ\", None)\n",
    "# api_url = f\"https://api-inference.huggingface.co/pipeline/feature-extraction/{model_id}\"\n",
    "# headers = {\"Authorization\": f\"Bearer {hf_token}\"}\n",
    "\n",
    "# def encode_texts(texts):\n",
    "#     \"\"\"\n",
    "#     テキストをエンコードする\n",
    "#     \"\"\"\n",
    "#     response = requests.post(\n",
    "#         api_url,\n",
    "#         headers=headers,\n",
    "#         json={\"inputs\": texts, \"options\": {\"wait_for_model\": True}},\n",
    "#     )\n",
    "#     return response.json()\n",
    "\n",
    "# tmp = pd.read_csv(\"../local/data/nhk_programs.csv\", dtype=str)\n",
    "# tmp = tmp.dropna()['title'].tolist()\n",
    "# tmp = encode_texts(tmp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
