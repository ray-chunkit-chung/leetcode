{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Prepare data\n",
    "\n",
    "- NHK program list text and genres (too few)\n",
    "- MARC-ja (not available)\n",
    "- JCoLA https://github.com/osekilab/JCoLA <-- we are using this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_nhk_programs(url: str) -> dict:\n",
    "#     \"\"\"\n",
    "#     NHK番組表を取得する\n",
    "#     \"\"\"\n",
    "\n",
    "#     try:\n",
    "#         response = requests.get(url)\n",
    "#         response.raise_for_status()\n",
    "#         data = response.json()\n",
    "#     except:\n",
    "#         data = None\n",
    "#     return data\n",
    "\n",
    "\n",
    "# def cast_json_to_pandas(json_data):\n",
    "#     \"\"\"\n",
    "#     JSONデータをPandasのDataFrameに変換する\n",
    "#     \"\"\"\n",
    "#     if json_data is None:\n",
    "#         return None\n",
    "#     return pd.json_normalize(json_data[\"list\"][\"g1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://api.nhk.or.jp/v2/pg/genre/{area}/{service}/{genre}/{date}.json?key={apikey}\n",
    "\n",
    "# apikey = os.environ.get(\"NHK_API_KEY\", None)\n",
    "# area = \"130\"  # 東京\n",
    "# service = \"g1\"  # NHK総合\n",
    "# start_date = \"2024-04-24\"  # 開始日\n",
    "# end_date = \"2024-11-24\"  # 終了日\n",
    "\n",
    "# get_list_of_dates = (\n",
    "#     lambda start_date, end_date: pd.date_range(start_date, end_date)\n",
    "#     .strftime(\"%Y-%m-%d\")\n",
    "#     .tolist()\n",
    "# )\n",
    "\n",
    "# dates = get_list_of_dates(start_date, end_date)\n",
    "\n",
    "# nhk_programs_json = map(\n",
    "#     lambda date: get_nhk_programs(\n",
    "#         f\"https://api.nhk.or.jp/v2/pg/list/{area}/{service}/{date}.json?key={apikey}\"\n",
    "#     ),\n",
    "#     dates,\n",
    "# )\n",
    "# nhk_programs_json = filter(lambda x: x is not None, nhk_programs_json)\n",
    "# df = pd.concat(map(cast_json_to_pandas, nhk_programs_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = \"../local/data/JCoLA-main/data/jcola-v1.0/in_domain_train-v1.0.tsv\"\n",
    "# df = pd.read_csv(data_path, sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Featrue engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name studio-ousia/luke-japanese-base-lite. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60bb7b516dd423eb334c5cb96664faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/873 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ray\\Desktop\\src\\leetcode\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ray\\.cache\\huggingface\\hub\\models--studio-ousia--luke-japanese-base-lite. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93fb9385abc94578bf3f01c5df39662d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/536M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66db96503f3c45bb8bb7929981570e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.22k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec7c087d1b147ec85d5e23246fed33d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/842k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf86a331e8fd4f90bb8fc1e4c69b5b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "entity_vocab.json:   0%|          | 0.00/51.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec09c4c599af4934afadea27e4ea473c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/33.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329c1e27bc1b4b3b85bad737e8601d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/468 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd4cfdff8a5411a9750341316eaf157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.57M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# os.makedirs(\"../local/data\", exist_ok=True)\n",
    "# df.to_csv(\"../local/data/nhk_programs.csv\", index=False)\n",
    "# df = pd.read_csv(\"../local/data/nhk_programs.csv\", dtype=str).dropna()\n",
    "\n",
    "data_path = \"../local/data/JCoLA-main/data/jcola-v1.0/in_domain_train-v1.0.tsv\"\n",
    "df = pd.read_csv(data_path, sep=\"\\t\")\n",
    "\n",
    "# Text feature extraction\n",
    "model_name = \"oshizo/sbert-jsnli-luke-japanese-base-lite\"\n",
    "# model_name = \"studio-ousia/luke-japanese-base-lite\"\n",
    "encoder = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "# text_features = [\"title\", \"subtitle\", \"content\", \"act\"]\n",
    "text_features = [\"sentence\"]\n",
    "\n",
    "for i in text_features:\n",
    "    embeddings = pd.DataFrame(\n",
    "        encoder.embed_documents(df[i]), index=df.index\n",
    "    ).add_prefix(f\"embed_{i}_\")\n",
    "    df = df.join(embeddings, how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embed_sentence_0</th>\n",
       "      <th>embed_sentence_1</th>\n",
       "      <th>embed_sentence_2</th>\n",
       "      <th>embed_sentence_3</th>\n",
       "      <th>embed_sentence_4</th>\n",
       "      <th>embed_sentence_5</th>\n",
       "      <th>embed_sentence_6</th>\n",
       "      <th>embed_sentence_7</th>\n",
       "      <th>embed_sentence_8</th>\n",
       "      <th>embed_sentence_9</th>\n",
       "      <th>...</th>\n",
       "      <th>embed_sentence_759</th>\n",
       "      <th>embed_sentence_760</th>\n",
       "      <th>embed_sentence_761</th>\n",
       "      <th>embed_sentence_762</th>\n",
       "      <th>embed_sentence_763</th>\n",
       "      <th>embed_sentence_764</th>\n",
       "      <th>embed_sentence_765</th>\n",
       "      <th>embed_sentence_766</th>\n",
       "      <th>embed_sentence_767</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.167172</td>\n",
       "      <td>-0.194901</td>\n",
       "      <td>-0.043595</td>\n",
       "      <td>-0.167724</td>\n",
       "      <td>-0.325008</td>\n",
       "      <td>-0.127364</td>\n",
       "      <td>0.030671</td>\n",
       "      <td>0.122342</td>\n",
       "      <td>-0.002697</td>\n",
       "      <td>-0.281892</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1669</td>\n",
       "      <td>0.089946</td>\n",
       "      <td>0.121289</td>\n",
       "      <td>-0.060072</td>\n",
       "      <td>-0.404347</td>\n",
       "      <td>0.070873</td>\n",
       "      <td>0.015921</td>\n",
       "      <td>0.176191</td>\n",
       "      <td>-0.096621</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   embed_sentence_0  embed_sentence_1  embed_sentence_2  embed_sentence_3  \\\n",
       "0          0.167172         -0.194901         -0.043595         -0.167724   \n",
       "\n",
       "   embed_sentence_4  embed_sentence_5  embed_sentence_6  embed_sentence_7  \\\n",
       "0         -0.325008         -0.127364          0.030671          0.122342   \n",
       "\n",
       "   embed_sentence_8  embed_sentence_9  ...  embed_sentence_759  \\\n",
       "0         -0.002697         -0.281892  ...             -0.1669   \n",
       "\n",
       "   embed_sentence_760  embed_sentence_761  embed_sentence_762  \\\n",
       "0            0.089946            0.121289           -0.060072   \n",
       "\n",
       "   embed_sentence_763  embed_sentence_764  embed_sentence_765  \\\n",
       "0           -0.404347            0.070873            0.015921   \n",
       "\n",
       "   embed_sentence_766  embed_sentence_767  label  \n",
       "0            0.176191           -0.096621      1  \n",
       "\n",
       "[1 rows x 769 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose training features\n",
    "# training_features = [i for i in df.columns if \"embed_\" in i]\n",
    "# df = df[training_features + [\"label\"]]\n",
    "\n",
    "# Target label data clearning\n",
    "# df[\"genres\"] = df[\"genres\"].map(eval)\n",
    "# df = df.explode(\"genres\").reset_index(drop=True)\n",
    "\n",
    "# Choose training features\n",
    "training_features = [i for i in df.columns if \"embed_\" in i]\n",
    "target_label = \"label\"\n",
    "df = df[training_features + [target_label]]\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score  0.8599819331526649\n",
      "test score  0.8258670520231214\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.14      0.21       236\n",
      "           1       0.85      0.97      0.90      1148\n",
      "\n",
      "    accuracy                           0.83      1384\n",
      "   macro avg       0.66      0.55      0.56      1384\n",
      "weighted avg       0.78      0.83      0.78      1384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# train classifier by logitic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X = df[training_features]\n",
    "y = df[target_label]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"train score \", clf.score(X_train, y_train))\n",
    "print(\"test score \", clf.score(X_test, y_test))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ray\\Desktop\\src\\leetcode\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\ray\\Desktop\\src\\leetcode\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4621, number of negative: 914\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049000 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 5535, number of used features: 768\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.834869 -> initscore=1.620536\n",
      "[LightGBM] [Info] Start training from score 1.620536\n",
      "train score  1.0\n",
      "test score  0.8323699421965318\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.11      0.19       236\n",
      "           1       0.84      0.98      0.91      1148\n",
      "\n",
      "    accuracy                           0.83      1384\n",
      "   macro avg       0.69      0.55      0.55      1384\n",
      "weighted avg       0.79      0.83      0.78      1384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train text classifier with light boost\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "clf = LGBMClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "print(\"train score \", clf.score(X_train, y_train))\n",
    "print(\"test score \", clf.score(X_test, y_test))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using HuggingFace api for feature extraction\n",
    "\n",
    "# model_id = \"oshizo/sbert-jsnli-luke-japanese-base-lite\"\n",
    "# hf_token = os.environ.get(\"HUGGINGFACE_API_KEY_READ\", None)\n",
    "# api_url = f\"https://api-inference.huggingface.co/pipeline/feature-extraction/{model_id}\"\n",
    "# headers = {\"Authorization\": f\"Bearer {hf_token}\"}\n",
    "\n",
    "# def encode_texts(texts):\n",
    "#     \"\"\"\n",
    "#     テキストをエンコードする\n",
    "#     \"\"\"\n",
    "#     response = requests.post(\n",
    "#         api_url,\n",
    "#         headers=headers,\n",
    "#         json={\"inputs\": texts, \"options\": {\"wait_for_model\": True}},\n",
    "#     )\n",
    "#     return response.json()\n",
    "\n",
    "# tmp = pd.read_csv(\"../local/data/nhk_programs.csv\", dtype=str)\n",
    "# tmp = tmp.dropna()['title'].tolist()\n",
    "# tmp = encode_texts(tmp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
